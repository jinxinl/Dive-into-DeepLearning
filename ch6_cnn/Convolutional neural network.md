# Convolutional neural network

[TOC]

之前介绍的多层感知机 `MLP` 属于前馈神经网络，对输入的处理是一层一层顺序进行的，忽略了非相邻像素间的关联，并且输入也是直接将图像数据展平成一维向量，破坏了图像的空间结构。

最优的方法是利用先验知识，即相近像素之间的相互关联性，从图像数据中学习得到有效的模型。

本章主要讲 `CNN` 的基本元素，包括（1）卷积层本身（2）填充 `padding` （3）步幅 `sride` （4）汇聚层 `pooling` （5）多通道 `channel` 。除此之外，还介绍了一个完整的、可运行的 `LeNet` 模型。



## 卷积层的意义 why conv

**全连接层的局限性** 

全连接层适合处理表格类的数据，行代表样本，列代表特征，我们使用 `MLP` 来找到特征之间的关联，但是不能事先假设任何与特征交互有关的先验结构。对于高维数据，这种缺少结构的网络并不实用。

以图像为例，每张图片高达百万像素，`MLP` 中全连接层的方式会面里非常大的参数开销，需要大量的GPU、分布式优化训练，操作麻烦，即使图片只有几千个像素，同样需要很多的计算资源。实际上，图像中本来就有很丰富的结构，只不过 `MLP` 的架构局限了对图像结构的利用。

*而卷积神经网络则是机器学习利用自然图像中一些已知结构的创造性方法*

**不变性** 

卷积神经网络利用了 *空间不变性* 的概念，将它系统化，从而实现了用少量参数学习有用的表示。

不变性，即空间不变性，是指我们基于常识就可以从一张图片中找到目标物体在哪，比如猪会在地上，云会在天上。在检测物体时，我们可以将图像划分为多个区域，对每一个子区域进行检测，为每一个子区域包含目标物体的可能性打分。

不变性可以分为：

- 平移不变性：目标物体不管出现在哪里，图像的上/下/左/右，网络的前几层针对相同图像区域都要做出相似的反应（打分）
- 局部性：神经网络的前几层只探索图像的局部区域，而不在意图像中相隔较远的区域，最终再聚合局部特征，进行图像级别的检测。

**数学表示**

输入矩阵是 $\bf X$ ，隐藏层矩阵是 $\bf H$ ，二者形状相同，假设 $\bf U$ 包含偏置项参数，$\bf W$ 是四阶权重张量
$$
\bf H_{i,j}=U_{i,j}+\sum_{k}\sum_{l}W_{i,j,k,l}X_{k,l}=U_{i,j}+\sum_{a}\sum_{b}V_{i,j,a,b}X_{i+a,j+b}
$$
从 $\bf U$ 到 $\bf V$ 只是形式上的转换，只要另 $\bf k=i+a,l=j+b$ 就可得到 $\bf U_{i,j,k,l}=V_{i,j,a,b}$ ，即索引 $a,b$ 通过偏移覆盖了整个图像，对于隐藏层 $\bf H$ 中任意位置 $(i,j)$ 都可以通过输入层 $\bf X$ 中以 $(i,j)$ 为中心对像素加权求和得到，权重矩阵是 $\bf V_{i,j,a,b}$ 

**平移不变性** （以相同的方式处理局部图像，与位置无关）

是指检测对象在输入 $\bf X$ 中的平移，仅导致隐藏层表示 $\bf H$ 中的平移，而不会导致权重的变化，即 $\bf V$ 和 $\bf U$ 不依赖于 $(i,j)$ 的值，因此 $\bf V_{i,j,a,b}$ 实际上等价于 $\bf V_{a,b}$ ，并且 $\bf U$ 是一个常数，因此 $\bf H$ 可以写成
$$
\bf H_{i,j}=u+\sum_{a}\sum_{b}V_{a,b}X_{i+a,j+b}
$$
上述就是 **卷积** ，使用系数 $\bf V_{a,b}$ 对位置 $(i,j)$ 附近的像素 $(i+a,j+b)$ 进行加权得到 $\bf H$ ，并且 $\bf V_{a,b}$ 的系数相较于 $\bf V_{i,j,a,b}$ 而言少很多，因为前者不与像素位置有关。*系数的减少就是一大进步* 

**局部性** （计算隐藏表示所需参数更少）

是指不应该过度关注与当前图像区域相距较远的区域，可以采用 *权重置为0* 的方法对 *$ |a|>\Delta \space or \space |b|>\Delta $*  的区域实现不关注。重写 $\bf H_{i,j}$ ：
$$
\bf H_{i,j}=u+\sum_{a=-\Delta}^{\Delta}\sum_{b=-\Delta}^{\Delta}V_{a,b}X_{i+a,j+b}
$$
上述的 $\bf H$ 即为卷积层，而卷积神经网络是包含卷积层的一类特殊网络，$\bf V$ 被称为卷积核 `convolutional kernel` 或 滤波器 `filter` ，或是卷积层的权重，是可学习的参数。

需要注意的是，当 $\Delta =0$ 时，相当于全连接层，卷积内核为每组通道独立地实现一个全连接层。

对于一个很小的局部区域，卷积神经网络的训练和 `MLP` 可能差异巨大，因为卷积层所需参数远远少于与全连接层，前者只需几百个参数，而后者可以达到数十亿级别。参数大幅减少的代价是，特征是平移不变的，并且每一层只包含局部信息。以上权重学习依赖于 **归纳偏置** ，当这种偏置与现实符合时，就能得到有效模型，并且泛化性也很不错，但若是偏置与现实不符，模型就难以拟合训练数据。

**卷积的数学定义** 

在数学中，两个函数 $f$ 与 $g$ 之间的卷积定义为
$$
(f*g)({\bf x})=\int f({\bf z})g({\bf x-z})d{\bf z}
$$
即其中一个函数先翻转，再平移 $\bf x$ 时，计算二者此时的重叠。

对于离散对象，卷积定义如下
$$
(f*g)(x)=\sum_{a}f(a)g(i-a)
$$
对于二维张量，定义如下
$$
(f*g)(i,j)=\sum_{a}\sum_{b}f(a,b)g(i-a,j-b)
$$
本质上与上述的隐藏层表示 $\bf H$ 的表达式一致。

**通道 `channel`**

对于一张 `RGB` 图片，图像数据并不是二维张量，而是由高度、宽度、颜色组成的三维张量，前两个轴与像素的空间位置有关，第三个轴可以看成是每个像素的多维表示，因此输入 $\bf X_{i,j}$ 相应地调整为 $\bf X_{i,j,k}$ ，权重 $\bf V_{a,b}$ 调整为 $\bf V_{a,b,c}$ ，并且隐藏表示 $\bf H$ 也最好是三维张量，也就是对每一个区域，采用一组隐藏表示而不是一个，一组隐藏表示可以想象为一组二维网格堆叠，即一系列具有二维张量的通道，这些通道有时也被称为 **特征映射** ，每个通道都会为后续层提供一组空间化的学习特征，比如一些识别边缘，一些识别纹理。 $\bf H$ 可以重写为
$$
\bf H_{i,j,d}=\sum_{a=-\Delta}^{\Delta}\sum_{b=-\Delta}^{\Delta}\sum_{c}V_{a,b,c,d}X_{i+a,j+b,c}
$$
其中 $d$ 表示输出通道，接着 $\bf H_{i,j,d}$ 也将会进入下一个卷积层。



## Convolutional layer

卷积层的实际应用

**互相关运算** 

确切的来说，卷积运算应该叫做互相关运算 `cross-correlation` ，在互相关运算中，卷积核与输入张量中对应区域相乘，从左上角开始，先行后列开始滑动，直到右下角结束，输出的张量形状为
$$
(n_w-k_w+1)\times (n_h-k_h+1)
$$
代码如下

```python
def corr2d(X,K):
    '''互相关运算'''
    h,w = K.shape
    Y = torch.zeros(X.shape[0] - h + 1,X.shape[1] - w + 1)
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            Y[i,j] = (X[i:i+h,j:j+w] * K).sum()
    return Y
```

**卷积层** 

卷积层对输入和卷积核进行互相关运算后，加上偏置项，得到输出。

训练的参数是卷积核权重与标量偏置项。

卷积层的代码实现如下

```python
class Conv2D(nn.Module):
    def __init__(self,kernel_size):
        super().__init__()
        # 初始化卷积核权重和标量偏置
        self.weight = nn.Parameter(torch.rand(kernel_size))
        self.bias = nn.Parameter(torch.zeros(1))
    def forward(self,X):
        return corr2d(X,self.weight) + self.bias
```

形状为 $(h,w)$ 的卷积核一般叫做 $k \times w$ 卷积核，带有 $k\times w$ 卷积核的卷积层可以叫做 $k\times w$ 卷积层。

**目标边缘检测** 

卷积层的简单应用

*见 `notebook` 的 `Convolutional layer`*

**学习卷积核** 

对于更加复杂的识别任务，比如检测弯曲的边缘，此时卷积核很难通过手动定义，需要学习由 $\bf X$ 生成 $\bf Y$ 的卷积核。

学习方式即为神经网络的训练流程：

1. 初始化卷积核权重
2. 更新卷积核参数
   1. 先前向传播得到输出结果
   2. 计算结果与真实 $\bf Y$ 之间的 `MSE` 
   3. 反向传播，利用梯度对卷积核参数进行更新

（忽略偏置）

具体训练代码如下：

```python
# 构造二维卷积层，1个channel，卷积核形状(1，2)
conv2d = nn.Conv2d(1,1,kernel_size=(1,2),bias=False)

# 二维卷积层使用思维输入和输出（batch_size, channel, height, width）
X = X.reshape(1,1,6,8)
Y = Y.reshape(1,1,6,7)
lr = 3e-2

for i in range(10):
    Y_hat = conv2d(X)
    #print(Y_hat.shape)
    l = (Y_hat - Y) ** 2
    conv2d.zero_grad()
    l.sum().backward()
    conv2d.weight.data[:] -= lr * conv2d.weight.grad
    
    if (i + 1) % 2 == 0:
        print(f"epoch {i+1}, loss {l.sum():.3f}")
```



**互相关运算与卷积核** 

互相关与卷积差别不大，卷积运算相当于将卷积核进行水平和垂直翻转，再进行互相关运算。

尽管严格来说二者不同，但是可以称互相关运算为卷积运算，卷积核上的权重张量叫做 *元素*

**特征映射和感受野** 

特征映射：输出的卷积层，它可被视为把一个输入映射到下一层维度的转换器

感受野：在卷积神经网络中，某一层的任意元素 $\bf x$ 的感受野是指它在进行前向传播时可能影响 $\bf x$ 计算的所有元素，感受野的大小可能大于输入的大小，因为还需要包含前面所有层的相关元素。所以若是一个特征图想要检测更广区域的输入特征，需要构建一个更深的网络，让感受野尽可能多。

<img src="C:\Users\xinling\AppData\Roaming\Typora\typora-user-images\image-20250305181131788.png" alt="image-20250305181131788" style="zoom:50%;" />

这里输出的 $19$ 的感受野是输入中四个阴影部分元素，为4，若是将输出作为输入 $\bf Y$ ，$19$ 作为元素 $z$ ，在下一层仍然使用该核函数，最后得到的结果是一个值，这个值的感受野为13，因为包含了 $\bf Y$ 上的四个元素和最开始 $\bf X$ 上的9个元素。



## 填充与步幅 padding and stride

填充和步幅可以有效调整数据维度

**填充** 

因为在输入 $\bf X$ 与卷积核做卷积运算时，边缘像素的信息会丢失，对于较小的卷积核，只经过一层卷积并没有太大影响，只会丢失几个像素，但是经过多层连续卷积之后，丢失的信息就会累加得越来越多。

解决方法是 *填充* ，通过在边界填充元素（一般填充0），可以将输入矩阵扩张，若是添加了 $p_h$ 行填充和 $p_w$ 列填充，那么输出的形状将是：
$$
(n_w-k_w+p_w+1)\times (n_h-k_h+p_h+1)
$$
大部分情况下，设置 $p_h=k_h-1,p_w=k_w-1$ 若是 $k_w$ 为奇数，左右两边各添加  $p_w/2$ 列，若是 $k_w$ 为偶数，一边添加 $\lceil p_h/2 \rceil$ ，一边添加 $\lfloor p_h/2 \rfloor$ ，一般是上面添加前者。

当然，在卷积神经网络中，卷积核的尺寸一般选择奇数，如1，3，5，7。奇数核也能提供书写上的便利

**步幅** 

卷积核滑动顺序是从左上到右下，默认每次滑动一个元素，会发现元素十分冗余，为了提高计算的效率，缩减采样次数，因此可以设置 *步长* ，让窗口一次跳过几个元素，跳过的元素数量就叫 *步长* 。

当水平步长为 $s_w$ ，垂直步长为 $s_h$ 时，输出的形状为：
$$
[(n_w-k_w+p_w+s_w)/s_h]\times [(n_h-k_h+p_h+s_h)/s_h]
$$
若是 $p_h=k_h-1,p_w=k_w-1$ ，则上述公式可进一步简化为：
$$
[(n_w+s_w-1)/s_w] \times [(n_h+s_h-1)/s_h]
$$


## 多输入多输出通道 channels

**多输入通道** 

前面讨论的卷积运算都是单个输入输出通道，但是对于RGB图像，有三个颜色，即三个通道，因此输入矩阵 $\bf X$ 实际上是三维张量，将大小为3的轴成为 *通道* 。

输入包含多通道时，假设通道数为 $c_i$ ，那么相应的卷积核也要有 $c_i$ 个输入通道，形状为 $c_i\times k_h\times k_w$ ，对每个通道的二维输入和卷积核做卷积运算，在将不同通道对应位置的元素相加求和，得到最终结果。

<img src="C:\Users\xinling\AppData\Roaming\Typora\typora-user-images\image-20250305213331501.png" alt="image-20250305213331501" style="zoom:67%;" />



**多输出通道** 

多输出通道十分重要，可以通过减少空间分辨率来增加输出通道数，将每个通道看成是对不同特征的响应，需要注意的是，每个通道不是单独进行优化的，而是为了共同使用而优化的，多输出通道并不仅是学习多个单通道的检测器。

为每个输出通道建立 $c_i\times k_h\times k_w$ 的卷积核，这样卷积核就是四维张量 $c_o\times c_i\times k_h\times k_w$ ，在互相关运算中，每个输出通道先获取所有输入通道，再以对应该通道的卷积核计算出结果。通俗来讲，就是一组多输入通道，分别与多组三维卷积核进行卷积运算，结对应于各自的输出通道，最后堆叠成多输出通道



**$1\times 1$ 卷积层**

虽然 $1\times 1$ 卷积层看起来没有太多意义，因为不能起到提取局部特征的作用，而卷积的本质又是“有效提取相邻像素的相关特征”，但是 $1\times 1$ 卷积层能够调整输入和输出的数据形状，主要是改变通道数，元素值不变。

<img src="C:\Users\xinling\AppData\Roaming\Typora\typora-user-images\image-20250305222404227.png" alt="image-20250305222404227" style="zoom:60%;" />

如上所示，在多输入多输出通道中， $1\times 1$ 卷积层能够将输出通道数压缩为2，卷积层的权重维度是 $c_o\times c_i$ ，再额外加上一个偏置。$1\times 1$ 卷积的 **唯一** 计算也就发生在通道上，没有起到卷积的效果，等价于是全连接层。



## 汇聚层 pooling

在之前的卷积层中，存在着一些问题：

- 最后一层神经元需要对全局图像进行识别，即需要对整个输入的全局敏感，因此需要不断聚合信息，最终实现全局表示的目标，将卷积层的所有优势留在中间层。然而对于很大的图像，若是只有一层一层卷积，会需要很多层卷积层，网络太大。
- 当检测一些底层特征（靠近输入层的特征）时，需要保持平移不变性，然而实际上，若是整个图像向右移动一个像素，都会导致输出截然不同，可是现实中即使移动像素，物体位置也不会发生太大改变，因为物体一般占据成百上千个像素。

因此需要引入汇聚层 `pooling` ，它通过 **降采样** 解决了：

- 通过降低空间分辨率来增大神经元对其敏感的感受野，降低对空间将采样的敏感性
- 降低卷积层对位置的敏感性

**最大汇聚层和平均汇聚层** 

汇聚层运算与卷积层类似，有一个固定形状的窗口，在图像上滑动，不同的是，卷积运算需要有卷积核等参数，而 **汇聚层运算不需要任何参数** 。

最大汇聚层，取图像中窗口内的最大值。

平均汇聚层，取图像中窗口内的平均值。

汇聚后的结果是，假设是 $2\times 2$ 的汇聚层，那么无论 $X[i,j]$ 与 $X[i+\Delta ,j+\Delta],\Delta\leq2$ 是否相等，汇聚层中对应结果都一样，降低了卷积层对位置的敏感性，将维度减少了 $2\times 2=4$ 倍

**填充与步幅** 

与卷积层一样，汇聚层也可以有填充与步幅，来改变输出的形状

使用最大汇聚层以及大于1的步幅，可以减少空间维度（如高度和宽度）

**多个通道** 

与卷积层不同，卷积层对于多输入通道，会将输入矩阵与对应卷积核进行卷积运算，再求和，输出通道数等于卷积核的组数。

汇聚层不求和，对每个通道单独运算，因此输入通道数与输出通道数相同



## LeNet

`LeNet` 是最早发布的卷积神经网络之一，在当时取得了与 `SVM` 相媲美的成果，广泛应用与银行ATM机，帮助处理支票手写数字识别的问题，提出者是 `Yann LeCun` 。

模型由两部分组成：

- 卷积编码器：两个卷积层
- 全连接层密集块：三个全连接层

<img src="C:\Users\xinling\AppData\Roaming\Typora\typora-user-images\image-20250307142651293.png" alt="image-20250307142651293" style="zoom:80%;" />

具体介绍如下：

- 每个卷积块是 `1` 个卷积层，`1` 个 $sigmoid$ 函数和 `1` 个平均汇聚层（因为1989还未出现 $ReLU$ 和最大汇聚层）
- 每个卷积层使用 $5\times 5$ 的卷积核和 `1` 个 $sigmoid$ 函数，通常增加通道数量，第 `1` 个卷积层有 `6` 个输出通道，第 `2` 个卷积层有 `16` 个输出通道。
- 每个汇聚层的滑动窗口大小为 $2\times 2$ ，步幅为 $2$ ，通过汇聚能将维度减少 $4$ 倍。
- 在将卷积层传递给全连接层稠密块时，必须展平每个样本，从四维输入 `(batch_size, channel, height, weight)` 展平为二维输入 `(batch_size, input_vector)` 。
- 三个全连接层的输出维度分别是 `120` `84` `10` ，最后的 `10` 个维度对应着手写数字分类中的 `10` 个类别。

<img src="C:\Users\xinling\AppData\Roaming\Typora\typora-user-images\image-20250307145942148.png" alt="image-20250307145942148" style="zoom:80%;" />

`LeNet` 实现见 `notebook`

