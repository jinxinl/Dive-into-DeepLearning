# Convolutional modern

[TOC]

尽管 `CNN` 在计算机视觉领域和机器学习领域很有名气，但是它并没有占主导地位，大家仍然使用 `SVM` 算法来解决问题。

早期机器学习的流水线是：（1）获取数据集（2）人工提取特征（3）训练分类器处理数据。深度网络在当时仍有很大的局限性，主要首先于以下两方面：

- 数据：对于深层神经网络，尽管使用的是卷积层，但是参数量仍然不小，需要大量的标注数据，否则会有严重的过拟合风险。
- 硬件：GPU的图形处理器能够加速图形处理，而其中的数学运算与卷积运算十分相似，通过将卷积网络的卷积和矩阵乘法移动到GPU上，神经网络效率显著提高，



## AlexNet

<img src="C:\Users\xinling\AppData\Roaming\Typora\typora-user-images\image-20250308151049262.png" alt="image-20250308151049262" style="zoom:67%;" />

- 与 `LeNet` 的不同：

  - 比 `LeNet` 更深，有八层：五个卷积层、两个全连接隐藏层和一个全连接输出层

  - `AlexNet` 使用 $ReLU$ 而非 $sigmoid$ （1） $ReLU$ 计算更加简单，因此效率更高，不用进行附复杂的求幂运算（2）$ReLU$ 在正区间的梯度是1，而$sigmoid$ 函数在输入接近0或1时，梯度接近于0，容易导致梯度消失，模型训练不动，而 $ReLU$ 很好地缓解了梯度消失的问题

    【`LeNet` 使用 $sigmoid$ 的原因是1989年 $ReLU$ 尚未被提出】

  - `AlexNet` 控制全连接层复杂度的方法是 `Dropout` ，而 `LeNet` 则是通过 `Weight decay` 
  - `AlexNet` 对训练数据进行了数据增强，包括图片翻转、裁剪、加噪等，使得模型更加健壮，更大的样本量有效防止过拟合

  - `AlexNet` 的卷积通道数是 `LeNet` 的 $10$ 倍

- 模型设计细节

  - 第一层卷积层的卷积核是 $11\times 11$ ，因为 `AlexNet` 使用的是 `ImageNet` 数据集，其中大部分图像是 `MNIST` 的数十倍，需要更大的卷积核来捕获目标。

    第二层卷积层的卷积核是 $5\times 5$ 

    第三层卷积层的卷积核是 $3\times 3$ 

  - 在第 `1` `2` `5`  层卷积层之后，都有一个最大汇聚层，窗口形状是 $3\times 3$ ，步幅 `stride` 是 $2$ 

  - 在最后一个卷积层之后，有两个全连接隐藏层，分别有 $4096$ 个输出，这两层一共拥有将近 $1GB$ 的参数，由于早期GPU显存有限，因此 `AlexNet` 采用双数据流设计，每个GPU只需要存储和计算模型一般的参数，但是随着硬件设备的进步，现在很少需要跨GPU分解模型了。

具体代码见 `notebook`



## Visual geometry group （VGG）

- 背景
  - 虽然 `AlexNet` 很有效果，但是缺少一个通用的块来指导设计新的网络
  - 神经网络架构的设计逐渐抽象，从单个神经元，到层，再到重复块。

- `VGG` 块
  - 经典卷积网络架构：（1）带填充以保持分辨率的卷积层（2）非线性激活函数，常用 $ReLU$ （3）汇聚层，常用最大汇聚层
  - 一个 `VGG` 块与之类似，由一系列卷积层组成，后面再加上下采样的最大汇聚层，在 `VGG` 原始论文中，卷积核是 $3\times 3$ ，填充 `padding` 是 $1$ ，汇聚层的窗口是 $2\times 2$ ，步幅 `stride` 是$2$，实现了每个汇聚层后分辨率减半。

<img src="C:\Users\xinling\AppData\Roaming\Typora\typora-user-images\image-20250308172247902.png" alt="image-20250308172247902" style="zoom:70%;" />

- `VGG` 网络

  `VGG` 神经网络连接 `n` 个 `VGG` 块，其中 `conv_arch` 是超参数，指定了每个 `VGG` 块的卷积层个数和输出通道数，全连接层与 `AlexNet` 相同。

​	原始 `VGG` 网络有 `5` 个卷积块，前两个块各有一个卷积层，后三个块包含两个卷积层。

​	第一个模块有 `64` 个输出通道，每个后续模块会将输出通道数量翻倍，直到到达 `52` 。

​	由于上述网络使用 `8` 个卷积层和 `3` 个全连接层，因此也叫 `VGG-11` .

具体代码见 `notebook` 



## NiN （Net in Net）

- 背景：

  - `AlexNet` 与 `VGG` 都是先通过一系列的卷积层和汇聚层提取空间结构特征，然后用全连接层对特征表征进行处理，二者对 `LeNet` 的主要改进在于扩大和加深了卷积块和全连接稠密块。

    然而，最后展平再使用全连接层实际上相当于放弃了表征的空间结构，并且全连接的参数两也很大。

  - `NiN` 提出了 `Net in Net` 的方法，使用 $1\times 1$ 卷积核代替全连接层，上一章提到 $1\times 1$ 卷积核无法捕捉相邻元素的特征，输入输出的分辨率不变，只改变输出通道数，可以用它来代替全连接层。

**`NiN` 块**

- 方法：

  - 使用 $1\times 1$ 的卷积核代替全连接层，把权重分配到空间位置，当成是在像素位置上其独立作用的全连接层。把空间中的单个像素（输入通道中对应位置的一组像素）视为样本，通道维度视为不同特征维度【 $1\times 1$ 卷积层 —— “逐像素全连接层”】
  - `NiN` 块从一个普通的卷积层开始，卷积核形状由用户指定，随后是两个 $1\times 1$ 卷积层，激活函数是 $ReLU$ 。
  - 与 `VGG` 区别：

  <img src="C:\Users\xinling\AppData\Roaming\Typora\typora-user-images\image-20250309134856555.png" alt="image-20250309134856555" style="zoom:60%;" />

**`NiN` 网络**

- `NiN` 网络是在 `AlexNet` 后不久提出的，使用的窗口形状也与它基本一致，卷积核形状分别是 $11\times 11$ 、 $5\times 5$ 、 $3\times 3$ ，输出通道数与 `AlexNet` 中的相同，每个 `NiN` 块后有一个最大汇聚层，窗口形状是 $3\times 3$ ，步幅是 $2$ 。

  与 `AlexNet` 不同的是，`NiN` 取消了全连接层，使用 $1\times 1$ 卷积核逐像素全连接来代替，在最后加上一层全局平均汇聚层，生成一个对数几率 `logits` 。

- 优点：减少了全连接层，大大减少了参数量

- 局限：逐像素操作，增加了训练时间

具体代码见 `notebook`



## GoogLeNet

- 背景
  - 从前的卷积核形状，小到 $1\times 1$ ，大到 $11\times 11$ ，不同组合的情况很多。
  - `GoogLeNet` 吸收了 `NiN` 中串联网络的思想，并进行改进，包含并行联结的网络，解决了 **什么样的卷积核最合适的问题**

**`Inception` 块** 

- 方法

  - `GoogLeNet` 的一个和主要观点是：**使用不同大小的卷积和组合是有利的**

  - `Inception` 块：基本的卷积块。前三条路径使用 $1\times 1$ 、 $3\times 3$ 、 $5\times 5$ 卷积，从不同空间大小提取信息。中间两条路径先执行 $1\times 1$ 卷积， 降低通道数，进而降低模型复杂性。第四条路径先使用 $3\times 3$ 最大汇聚层，再使用 $1\times 1$ 卷积降低通道数。四条路经都通过 `padding` 和 `stride` 来使输出分辨率不变。

    最后在输出通道上连结，构成 `Inception` 块的输出。

    超参数一般是每层输出通道数。

  <img src="C:\Users\xinling\AppData\Roaming\Typora\typora-user-images\image-20250309153547586.png" alt="image-20250309153547586" style="zoom:67%;" />

**`GoogLeNet` 模型** 

- `9` 个 `Inception` 块和 `1` 个全局平均汇聚层堆叠。`Inception` 块之间的最大汇聚层可降低维度，继承自 `VGG` 块，全局平均汇聚层能够避免使用全连接层

  <img src="C:\Users\xinling\AppData\Roaming\Typora\typora-user-images\image-20250309153816138.png" alt="image-20250309153816138" style="zoom:67%;" />

- 有效的原因：

  - 不同尺寸卷积核的组合，用各种卷积核尺寸探索图像，而不同大小的卷积核可以识别不同范围的图像细节
  - 为不同卷积核分配不同数量的参数



## 批量规范化 `batch normalization` 

`batch normalization` 是训练深层神经网络时，有效加速收敛速度的一种方法。

- 实际挑战：

  - 数据预处理方式会对最终结果产生巨大影响，在将数据传入模型开始训练前，第一步是将数据标准化，即 ${\bf x}=\frac{{\bf x-\mu}}{\bf \sigma}$ ，这样做可以将参数的量级统一，很好地与优化器配合使用。

  - 对于多层网络，在计算过程中，中间隐藏层变量可能会有很大的变化范围，不论是在不同层间，还是同层的不同单元，都有可能会出现参数值量级差别较大的情况，`batch normalization` 的提出者非正式地假设，**这些变量分布中的这种偏移可能会阻碍网络收敛** 。

    若是参数值相差过大，那么得到的梯度值也会有比较大的区别，那么在反向传播更新梯度时，就需要对学习率进行补偿调整，对收敛速度必然会有影响。

  - 更深层的网络容易过拟合，更需要正则化 `normalization` 

- 原理：在每次迭代中，首先对输入的批量数据进行规范化
  $$
  \bf BN(x)=\gamma \odot \frac{x-\hat{\mu_B}}{\hat{\sigma_B}}+\beta
  $$
  其中 $\bf x$ 是该批量的数据样本，$\bf \hat{\mu_B}$ 与 $\bf \hat{\sigma_B}$ 分别是当前批量的均值和方差，$\bf \gamma$ 和 $\bf \beta$ 是拉伸参数 `scale` 和偏移参数 `shift` ，形状与 $\bf x$ 相同，是需要学习的参数。

  $\bf \hat{\mu_B}$ 与 $\bf \hat{\sigma_B}$ 计算公式如下
  $$
  \bf \hat{\mu_B}=\frac{1}{|B|}\sum_{x\in B}x \\
  \bf \hat{\sigma_B}=\frac{1}{|B|}\sum_{x\in B}(x-\hat{\mu_B})^2+\epsilon
  $$
  其中，$\epsilon$ 是噪声，以确保永远不会除以 $0$ ，估计值 $\bf \hat{\mu_B}$ 与 $\bf \hat{\sigma_B}$ 用平均值和方差的噪声来抵消缩放问题，乍一看噪声是一个问题，但是实际上是有益的，优化中的各种噪声能够加速训练、减少过拟合，但是还没有理论证明。

  通过批量规范化，能够十分有效地将每一个中间层的输出值统一到同一量级，加速模型收敛。

-  需要注意的是，训练阶段和测试阶段的批量规范化方法、功能略有不同。在训练集中，无法准确计算整个数据分布的平均值和方差，因此训练阶段批量规范化所使用的均值和方差是基于小批量数据的，是估计值。测试阶段因为可以看到整个数据集，所以使用的是整个数据集的均值和方差，是精确值。

- 实践：

  - 全连接层：`batch normalization` 是应用于全连接层的仿射变换和激活函数之间
    $$
    \bf h=\Phi(BN(Wx+b))
    $$
    均值和方差是在小批量数据上计算的

  - 卷积层：`batch normalization` 是在卷积层和激活函数之间的，当卷积层有多个输出通道时，对所有输出通道进行统一的批量规范化，计算的平均值和方差是基于 $channels\times height\times weight$ 的所有像素，对给定的没个输出通道均应用相同的方差和均值，对每个空间位置的值进行规范化。
  
  - 预测过程：不再需要均值中的噪声和小批次上的样本方差。因为需要使用模型对样本逐个预测，一个常用的方法是 **移动平均法** ，来估算整个训练数据集的样本均值和方差，并在预测时使用它们得到准确输出。
  
  【和暂退法一样，训练阶段和预测阶段得到的结果是不一样的】

- 争议：
  - 可解释性还是不够，理论依据也不够，作者的说法是通过减少 **内部协变量偏移** ，但是这里的 **内部协变量偏移** 就类似一种投机直觉，直觉和技术是不一样的，直觉不能解释为什么技术如此有效，缺少理论依据。
  - 尽管如此，`batch normalization` 仍然是一项有效且流行的技术，获得了 *2017年 $NeurIPS$ 的 “接收时间考验奖”*  。



## 残差网络 `ResNet`

使用数学基础知识，让神经网络取得质的突破

- 背景：假设模型架构是 $\mathcal{F}$ ，我们使用数据训练模型是为了得到真正想要的函数 $f$ ，但是不一定有 $f\in \mathcal{F}$ ，因此我们选择用 $f^*\in \mathcal{F}$ 近似 $f$ 。因此如何得到 $f^*$ 是一个关键问题。

  唯一的方法是设计一个更强大的架构 $\mathcal{F^*}$ ，并且预计 $f^*\in \mathcal{F^*}$ 的近似效果优于 $f\in \mathcal{F}$ ，但是无法保证新的体系更加接近。对于非嵌套函数，较复杂的函数并不总是像目标靠拢。

  解决方法：使用嵌套函数

  <img src="C:\Users\xinling\AppData\Roaming\Typora\typora-user-images\image-20250311170438224.png" alt="image-20250311170438224" style="zoom:67%;" />

- 原理：每个附加层都更简单地包含原始函数，作为其元素之一。将新添加的层训练成恒等映射， $f({\bf x})=\bf x$ 。新旧模型同样有效，并且新模型可能得出更优的解训练数据集，能够降低训练误差，同时也极大缓解了梯度消失问题

**残差块**

沿用了 `VGG` 的 $3\times 3$ 卷积层设计，现有两个相同输出通道数的 $3\times 3$ 卷积层，每个卷积层之后有一个 $BatchNorm$ 层 和 $ReLU$ 函数，然后通过跨层数据通路，跳过两个卷积运算，直接将输入加在 $ReLU$ 函数之前。

<img src="C:\Users\xinling\AppData\Roaming\Typora\typora-user-images\image-20250311172536691.png" alt="image-20250311172536691" style="zoom:67%;" />

可以额外使用 $1\times 1$ 卷积层改变输出通道数。

<img src="C:\Users\xinling\AppData\Roaming\Typora\typora-user-images\image-20250311191912072.png" alt="image-20250311191912072" style="zoom:67%;" />

**`ResNet` 模型** 

`ResNet` 的前两层与 `GoogLeNet` 一样：输出通道数为 `64` ，步幅为 `2` 的 $7\times 7$ 卷积层后接步幅为 `2` 的 $3\times 3$ 最大汇聚层，不同的是 `ResNet` 还加上了 `batch normalization` 层。

`GoogLeNet` 在卷积层之后加上了 `4` 个 `Inception` 模块，而 `ResNet` 则是加上了 `4` 个由残差块组成的模块，每个模块中包含 `2` 个残差块，第一个模块的输入输出通道数相同，因为在此之前已经使用了步幅为 `2` 的最大汇聚层，因此无需减少高和宽，之后的每一个模块在第一个残差块里将上一个模块的通道数翻倍，并将高度和宽度减半。最后加上全局平均汇聚层。

**`ResNet-18`**

每个模块 `4` 个卷积层（不含恒等映射的 $1\times 1$ 卷积层），加上第一个 $7\times 7$ 卷积层和最后一个全局平均汇聚层，一共 $4\times 4+2=18$ 层。

通过配置不同的通道数和模块中的残差块数目，可以得到不同的 `ResNet` 模型，比如有 `152` 层的 `ResNet-152` ，与 `GoogLeNet` 相比，`ResNet` 虽然架构与它相似，但是更容易修改，也更简单，因而被广泛使用

<img src="C:\Users\xinling\AppData\Roaming\Typora\typora-user-images\image-20250311193329269.png" alt="image-20250311193329269" style="zoom:67%;" />

具体实现过程见 `notebook` 



## DenseNet

`DenseNet` 相当于 `ResNet` 的改良版，二者的区别在于：`DenseNet` 是连接，而 `ResNet` 只是简单的相加。

<img src="C:\Users\xinling\AppData\Roaming\Typora\typora-user-images\image-20250311222916221.png" alt="image-20250311222916221" style="zoom:60%;" />

从泰勒展开的角度：

$f(x)=f(0)+f'(0)x+\frac{f''(0)}{2!}x^2+\frac{f'''(0)}{3!}x^3+...$ 

`ResNet` 将函数展开为：$f(x)=x+g(x)$ ，即一个简单的线性项和一个复杂的非线性项。

`DenseNet` 是这种分解方法的再往前一步，它将 $f$ 分解为多部分信息，即：$x\rightarrow [x,f_1(x),f_2([x,f_1(x)]),f_3([x,f_1(x),f_2([x,f_1(x)])])]$ ，最后将这些展开式结合到 `MLP` 中，再次减少特征数量。

**实现**

`DenseNet` 实现起来并不复杂，并不需要添加额外内容，只需要进行“连接”，这也是 `DenseNet` 名字的由来——变量之间的稠密连接，最后一层能够与前面所有层紧密相连。

<img src="C:\Users\xinling\AppData\Roaming\Typora\typora-user-images\image-20250312094007247.png" alt="image-20250312094007247" style="zoom:60%;" />

稠密网络主要由 `2` 部分构成：稠密块 `dense block` 和过渡层 `transition layer` ，前者定义如何连接输入输出，后者控制通道数量，使其不会太复杂。