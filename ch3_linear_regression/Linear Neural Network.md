# Linear Neural Network

[TOC]

## Linear regression

*回归* 任务是探讨 `n` 个自变量【输入】与因变量【输出】之间的关系。常与 *预测* 有关。

**`linear regression` 基本元素：**

- 假设：（1）自变量 **`x`** 与因变量 `y` 之间的关系是线性的，这里通常允许观测值存在噪声（2）假设噪声是正常的，如遵循正态分布
- 特征 `feature` / 协变量 `covariate` ：自变量 **`x`**
- 目标 `target` / 标签 `label` ：因变量 `y`
- 一般用 `n` 表示数据集总数，对第 `i` 个样本，输入表示为 $x^{(i)}=[x_1^{(i)},x_2^{(i)}]^T$ ，对应标签 $y^{(i)}$ 

**`Linear model`**

`linear model` 是指目标 `y` 是特征 `x` 的加权和

-  `function` ：$\hat{y}=\sum_{i=1}^{d}w_{i}\cdot x_{i}=\bf w^T\bf x+b=\bf Xw+b$ 
  - $\hat{y}=\sum_{i=1}^{d}w_{i}\cdot x_{i}$ ：加权和
  - $\hat{y}=\bf w^T\bf x+b$ ：$\bf x$ 是单个数据样本的特征，
  - $\hat{y}=\bf Xw+b$ ：$\bf X$ 是特征集合，每一行是一个数据样本，每一列是一个特征。这个过程将使用广播机制（因为 $\bf w$ 是列向量，形状是 $d\times 1$ ，而 $\bf X$ 的形状是 $n\times d$ ，虽然不满足矩阵-向量积的条件，但是满足广播机制的条件 ）
- `linear model` 的目标就是找到好的模型参数 `w` `b` ，需要
  - 模型质量的度量方式
  - 更新模型的方式，来提高模型质量

**损失函数——度量方式**

选择标准

- 一般选择一个非负的数作为损失，数值越小损失越小，回归问题中常用平方误差 `MSE` ，分类问题中常用交叉熵 `cross-entropy` 
- `MSE` ：$L=\sum_{i=1}^{n}\frac{1}{2}(\hat{y}^{(i)}-y^{(i)})^{2}$ , $\bf w^*,b^*=\arg\min_{\bf w,b}L$ 

**梯度下降——优化方式**

​	对于上述的平方误差，即使 $\bf X$ 不是满秩矩阵【`e.g.` 数据总量很多但是特征只有一些】，也能够使用 **最小二乘法** 得到解析解
$$
\bf w^*=(\bf X^T\bf X)^{-1}\bf X^T\bf y
$$
​	但是线性问题在深度学习中属于基础的、简单的问题，还有许多更加复杂的问题，通常没办法直接求得解析解，或者是根本不存在解析解，而 **梯度下降** 则是更加通用的进行优化的方式。

- 含义：通过计算损失函数的梯度来更新参数

- 具体方法：在数据集中随机抽取小批量的数据，计算该批量的平均损失关于模型参数的导数（梯度），再将计算得到的梯度乘上学习率 $\eta$ ，并将其从当前参数值中减掉
  $$
  \bf w=\bf w-\eta\sum_{i=1}^{batch\_size}\partial_{\bf w}l^{(i)}
  $$
  $b$ 的更新公式类似

  需要注意的是，$\bf w$ 与 $b$ 一定要同时更新，不是先后更新。

  此外，类似 $\eta$ $batch\_size$ 这些都属于深度学习中的超参数，需要事先手动指定，对模型性能和训练效果起重要影响

**矢量化加速**

- 具体内容：同时处理整个小批量样本，因此需要对计算进行矢量化，利用线性代数库，而不是使用类似 `for` 循环按序处理。

- 优势：矢量化代码通常会带来数量级的加速

  ```python
  a = torch.ones()
  b = torch.ones()
  c = a + b 
  # 而不是
  #for i in range(10):
  	#c[i] = a[i] + b[i]
  ```



**正态分布与平方损失** （从数学角度解释为什么平方误差可以用于线性回归的损失函数中，与极大似然概率有关）

`MSE` 可以用于线性回归的一个原因是：假设观测中包含噪声，并且噪声是正常的，服从正态分布
$$
{\bf y=w^Tx}+b+\epsilon,\epsilon\sim \mathcal{N}(0,\sigma^2)
$$
于是可以通过 $\bf x$ 得到 $\bf y$ 的似然
$$
P({\bf y|X})=\frac{1}{\sqrt{2\pi\sigma^2}}exp(-\frac{1}{2\sigma^2}({\bf y-w^Tx}-b)^2)
$$
所以最优的参数实际上是使 $\bf y$ 的似然最大的参数，
$$
P({\bf y|X})=\sum_{i=1}^{n}p(\bf y^{(i)}|x^{(i)})
$$
又因为一般是最小化损失函数，所以在前面加一个负号，就变成了最小化的问题。
$$
-logP({\bf y|X})=\sum_{i=1}^{n}\frac{1}{2}log(2\pi\sigma^2)+\frac{1}{2\sigma^2}({\bf y^{i}-w^Tx^{(i)}}-b)^2
$$
只要将前面一项的 $\sigma$ 看成是常数，第二项基本与 `MSE` 一致，因此在高斯噪声的假设下，最小化均方误差等价于对线性模型的极大似然估计。

*对于 $y=\sum_{i=1}^{n}w_ix_i+b$ 的，可以看成是只有一层的神经网络，即单层神经网络。并且每个特征对应有一个权重，因此也叫全连接层 `fully-connected network` 或稠密层 `dense layer`* 



## Softmax回归

根据给定特征 $\bf X$ 计算标签

标签分为：

- 硬标签 `hard label` ：只输出一个类别
- 软标签 `soft label` ：输出该样本属于每个类别的概率

表示输出类别的方法：独热编码 `one-hot encoding` ，所以最后输出的维度与类别数相同
$$
{\bf y}=(y_1,y_2,y_3)
$$

$$
y_i=\sum_{j=1}^{n}w_{ij}x_{j}+b_i
$$

<img src="C:\Users\xinling\AppData\Roaming\Typora\typora-user-images\image-20250227222945409.png" alt="image-20250227222945409" style="zoom:70%;" />

**全连接层的参数开销**

对于任何具有 $d$ 个输入和 $q$ 个输出的全连接层，参数开销为 $O(dq)$ ，开销较大，但实际中中可以降至 $O(\frac{dq}{n})$，`n` 是超参数，用于在实际应用中平衡参数。

**softmax运算**

- 输出 $\bf y$ 是一个维度等于类别数的向量，每个分量代表属于对应类别的概率，因此分量和为1。不能够将未经规范化的预测 $\bf y$ 作为输出，存在如下问题：

  - 分量和不一定为1
  - 分量不一定非负

  上述这两个问题都违背了概率论的基本公理

-  `softmax` 函数能够将输出规范为（1）分量和为1（2）各个分量都是非负，同时能够保证（3）模型的可导性，能够利用梯度下降进行优化。
  $$
  {\bf \hat{y}}=softmax({\bf o}),{\bf \hat{y}_j}=\frac{exp(x_j)}{\sum_{i=1}^{n}exp(x_i)}
  $$
  最后最大分量对应的类别即为预测的类别。
  $$
  output=\arg\max_j {\bf \hat{y}_j}
  $$

​	虽然`softmax` 是非线性函数，但是softmax回归的输出仍由输入特征的仿射变换决定，因此softmax回归是	线性回归

**小批量样本的矢量化加速**

为了提高GPU的利用率，加速计算，通常会对小批量的样本进行矢量化加速，进行 *矩阵-向量积* 乘法与 *矩阵加法* ，小批量样本的特征 $\bf X\in \mathbb{R}^{n\times d}$ ，权重 $\bf w\in \mathbb{R}^{d\times q}$ ，偏置 $b\in \mathbb{R}^{1\times q}$ 。
$$
{\bf O=XW+b,Y}=softmax({\bf O})
$$
所以在进行求和时会使用广播机制，

**损失函数**

与 `linear regression` 一样，使用最大似然估计，损失函数叫做交叉熵损失函数 `cross-etropy` 

似然估计：
$$
P({\bf Y|X})=\prod_{ii=1}^{n}P({\bf y^{(i)}|x^{(i)}})
$$
然后取负对数，最小化负对数似然，得到损失函数为
$$
L({\bf \hat{y},y})=-\sum_{i=1}^{n}{\bf y^{(i)}}log({\bf \hat{y}^{(i)}})
$$
因为所有 $\bf \hat{y}^{(i)}$ 都是概率，并且也不可能出现 $P({\bf y^{(i)}|x^{(i)}})=1$ ，因为数据集中往往存在一些噪声，比如标签误标，或者输入的特征没办法完美地对应每一个类别，所以 $log \bf \hat{y}^{(i)}$ 不会大于0。

将 ${\bf \hat{y}}=softmax({\bf o}),{\bf \hat{y}_j}=\frac{exp(x_j)}{\sum_{i=1}^{n}exp(x_i)}$ 代入上式 $L(\bf\hat{y},y)$ ，并进行求导，利用对数运算的性质，最后得到
$$
\partial_{o_j}l=softmax(o)_j-y_j
$$
也就是说，导数是sofmax模型分配的概率与实际观测之间的差异，迂回贵州看到的类似，梯度是真实值与预测值之间的差异。这并不是巧合，在任何指数族分布模型中，对数似然的梯度就是由此得出的

**信息熵**

分布 `P` 的熵：
$$
H(P)=-\sum_{i=1}^{n}P_ilogP_i
$$
信息论的基本定理指出，为了对从分布 `p` 中随机抽取的数据进行编码，至少需要 `H[P]` 纳特的数据。

纳特 `nat` 可以类比为比特 `bit` ，只不过比特是以 `2` 为底的对数，纳特是以 `e` 为底的对数，所以换算关系为：
$$
1nat=\frac{1}{log2}bit≈1.44bit
$$
信息论一个重要的概念就是信息量，简单理解就是对于从已知的事实预测接下来会发生的事，越有把握预测，说明预测结果所携带的信息量越低。我们面对接下来事件发生时的 *“惊异”* 程度，就反映了该事件所包含的信息量，Shannon决定用 $log\frac{1}{p_i}$  量化惊异程度。当事件发生的概率 $p_j$ 越小时，我们的惊异程度就会越大，该事件包含的信息量也越大。

用信息论的角度解释交叉熵，记交叉熵是从事件 $P$ 到事件 $Q$ ，即 $H(P,Q)$ ，可以理解为 *主观概率为 $Q$ 的观察者在看到根据概率 $P$ 生成的数据时的惊异程度* 。当 $P=Q$ 时，惊异程度最低，$H(P,P)=H(P)$ 。

我们使用交叉熵损失函数的优化目标是（1）最大化观测数据的似然（2）最小化传达标签所需的惊异。二者本质上是一样的



## `Linear regression` + `Softmax regression` 实践

`Download/pytorch/d2l-zh/pytorch/MyProject/linear_regression`

`Download/pytorch/d2l-zh/pytorch/MyProject/softmax_regression`
